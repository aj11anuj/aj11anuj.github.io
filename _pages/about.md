---
layout: about
title: About
permalink: /
subtitle: AI Engineering & Research
profile:
  align: right
  image: DP.jpg
  image_circular: false
  more_info: >
    <p>Delhi, India (Remote)</p>
selected_papers: false # includes a list of papers marked as "selected={true}"
social: false # includes social icons at the bottom of the page
announcements:
  enabled: false # includes a list of news items
  scrollable: false # adds a vertical scroll bar if there are more than 3 news items
  limit: 
latest_posts:
  enabled: false
  scrollable: true # adds a vertical scroll bar if there are more than 3 new posts items
  limit: 
---
<p style="text-align: justify;"> Pre-final year <a href="https://en.wikipedia.org/wiki/Bachelor_of_Technology" target="_blank" title="Bachelor of Technology">B.Tech</a> undergrad in Artificial Intelligence at <a href="https://www.niet.co.in/" target="_blank" title="NIET">Noida Institute of Engineering and Technology</a>, researching under PhD researcher <a href="https://www.researchgate.net/profile/Inam-Haq-14" target="_blank" title="Prof. Inam Ul Haq">Inam Ul Haq</a>. Iâ€™m driven by the ambition to create real-world impact through technology. I believe growth happens through rapid experimentation and hands-on execution. My curiosity has led me to explore and work across multiple domains from implementing ML models to prototyping MVPs. I approach challenges through practical experimentation, research, and iterative problem-solving. </p>

**_Open to research internships and collaborative research opportunities._**
<br><br>
#### Research Interests
**Large Language Models (LLMs)**, **Metaheuristic Algorithms**, **Multimodality**, and **Multilingual NLP**.
<br><br>
#### Publications
##### 2025
1. <span class="badge badge-primary">Preprint</span>  
   **Comparative Evaluation of Nature-Inspired Algorithms for Hyperparameter Optimization in Machine Learning Models**  
   Anuj Tiwari, Inam Ul Haq, Pramod Singh Rathore, and Abhishek Kumar  
   _Under review at Discover Computing journal of Springer Nature_
      <details style="display:inline;">
     <summary class="btn btn-sm btn-outline-light" style="display:inline-block; margin-right:0.5rem; cursor:pointer;">
       Abstract
     </summary>
     <div style="margin-top:0.5rem; max-width: 60rem;">
       <p style="margin-bottom:0;">
         Effective hyperparameter optimization is a common method for improving the performance of machine learning models
         and is a key aspect of enhancing their efficiency and generalization ability. In this work, we investigate the
         hyperparameter optimization of two popular models, Random Forest and Support Vector Machine (SVM), using three
         nature-inspired optimization algorithms: Bald Eagle Optimization (BEO), Particle Swarm Optimization (PSO), and
         Mother Tree Optimization (MTO). We evaluate these algorithms across five different datasets, including both image
         and tabular data, to rigorously assess their performance and the transferability of the findings across diverse
         settings. Our results show that Optuna delivers predictable and stable performance across a variety of evaluation
         metrics, particularly in balancing accuracy and computational efficiency, while the nature-inspired optimizers
         exhibit dataset-specific behavior, performing strongly in certain scenarios but inconsistently overall. The study
         highlights that no single optimizer can be universally recommended; instead, each method offers distinct advantages
         depending on dataset characteristics and model requirements, providing practical insights for real-world
         hyperparameter tuning in machine learning.
       </p>
     </div>
   </details>
   <a href="https://www.researchgate.net/publication/396803191_How_Effective_are_Nature-Inspired_Optimisation_Techniques_in_Hyperparameter_Tuning_of_Machine_Learning_Models" target="_blank" class="btn btn-sm btn-outline-light">Preprint</a>
